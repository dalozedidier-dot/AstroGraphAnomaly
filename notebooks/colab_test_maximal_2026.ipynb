{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AstroGraphAnomaly - Test maximal des capacites (Colab 2026)\n",
        "\n",
        "Ce notebook sert a stresser au maximum la pile: chargement artefacts, cross-match SIMBAD, visualisations Plotly interactives, embeddings UMAP et t-SNE, export PNG HD via kaleido, et (optionnel) un smoke-test torch-geometric si un graphe est disponible.\n",
        "\n",
        "Hypotheses:\n",
        "- Tu as deja un run AstroGraphAnomaly qui a produit raw.csv, scored.csv, top_anomalies.csv, et idealement graph_full.graphml.\n",
        "- Sinon, tu peux uploader ces fichiers ou pointer un dossier Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1) Installation des dependances (Colab)\n",
        "# Note: torch-geometric peut etre fragile selon CUDA. Ce bloc essaye une install robuste.\n",
        "!pip -q install --upgrade pip\n",
        "\n",
        "# Base data + viz\n",
        "!pip -q install -U pandas numpy matplotlib plotly kaleido astropy astroquery ipywidgets scikit-learn umap-learn networkx\n",
        "\n",
        "# Torch (garde une version pip standard)\n",
        "!pip -q install -U torch\n",
        "\n",
        "# Tentative torch-geometric (best effort)\n",
        "import os, sys, re, subprocess\n",
        "\n",
        "def _run(cmd: str) -> int:\n",
        "    print(\"+\", cmd)\n",
        "    return subprocess.call(cmd, shell=True)\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    torch_ver = re.match(r\"^(\\d+\\.\\d+\\.\\d+)\", torch.__version__).group(1)\n",
        "    cuda_ver = torch.version.cuda\n",
        "    if cuda_ver:\n",
        "        cuda_tag = \"cu\" + cuda_ver.replace(\".\", \"\")\n",
        "    else:\n",
        "        cuda_tag = \"cpu\"\n",
        "\n",
        "    wheel_url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
        "    # Wheels pyg\n",
        "    _run(f\"pip -q install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv -f {wheel_url}\")\n",
        "    _run(\"pip -q install torch-geometric\")\n",
        "    print(\"torch-geometric: OK\")\n",
        "except Exception as e:\n",
        "    print(\"torch-geometric: non installe (best effort). Erreur:\", repr(e))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2) Imports essentiels\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from astroquery.simbad import Simbad\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "# Optionnel: torch-geometric\n",
        "try:\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "    from torch_geometric.nn import GATConv\n",
        "    HAS_PYG = True\n",
        "except Exception:\n",
        "    HAS_PYG = False\n",
        "\n",
        "print(\"HAS_PYG =\", HAS_PYG)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3) Dossiers de sortie\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "os.makedirs(\"screenshots\", exist_ok=True)\n",
        "os.makedirs(\"screenshots_interactive\", exist_ok=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chargement des donnees\n",
        "\n",
        "Tu as trois options:\n",
        "1) Uploader raw.csv, scored.csv, top_anomalies.csv (et eventuellement graph_full.graphml) dans l'espace Colab.\n",
        "2) Pointer un dossier Drive.\n",
        "3) Si rien n'est dispo, le notebook tourne en mode \"df_top sample\" (limite) pour tester SIMBAD + 2-3 figures.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option 1: Upload manuel (decommente)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Option 2: Depuis Drive (decommente et adapte)\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "# base_dir = \"/content/drive/MyDrive/ton_dossier\"\n",
        "# raw_path = os.path.join(base_dir, \"raw.csv\")\n",
        "# scored_path = os.path.join(base_dir, \"scored.csv\")\n",
        "# top_path = os.path.join(base_dir, \"top_anomalies.csv\")\n",
        "# graph_path = os.path.join(base_dir, \"graph_full.graphml\")\n",
        "\n",
        "raw_path = \"raw.csv\"\n",
        "scored_path = \"scored.csv\"\n",
        "top_path = \"top_anomalies.csv\"\n",
        "graph_path = \"graph_full.graphml\"\n",
        "\n",
        "df_raw = None\n",
        "df_scored = None\n",
        "df_top = None\n",
        "graph = None\n",
        "\n",
        "if os.path.exists(top_path):\n",
        "    df_top = pd.read_csv(top_path)\n",
        "    print(\"OK top_anomalies.csv charge:\", df_top.shape)\n",
        "else:\n",
        "    # Mode demo rapide: top 10 exemple (a remplacer)\n",
        "    top_data = [\n",
        "        {\"source_id\": \"10000000922\", \"ra\": 266.4289451722255, \"dec\": -29.09800019698821, \"anomaly_score\": 0.7067},\n",
        "        {\"source_id\": \"10000000570\", \"ra\": 266.115474548117,   \"dec\": -29.02645189019288, \"anomaly_score\": 0.6745},\n",
        "    ]\n",
        "    df_top = pd.DataFrame(top_data)\n",
        "    print(\"ATTENTION: top_anomalies.csv absent, utilisation df_top sample\")\n",
        "\n",
        "if os.path.exists(scored_path):\n",
        "    df_scored = pd.read_csv(scored_path)\n",
        "    print(\"OK scored.csv charge:\", df_scored.shape)\n",
        "else:\n",
        "    print(\"scored.csv absent\")\n",
        "\n",
        "if os.path.exists(raw_path):\n",
        "    df_raw = pd.read_csv(raw_path)\n",
        "    print(\"OK raw.csv charge:\", df_raw.shape)\n",
        "else:\n",
        "    print(\"raw.csv absent\")\n",
        "\n",
        "if os.path.exists(graph_path):\n",
        "    try:\n",
        "        graph = nx.read_graphml(graph_path)\n",
        "        print(\"OK graph_full.graphml charge:\", graph.number_of_nodes(), \"nodes,\", graph.number_of_edges(), \"edges\")\n",
        "    except Exception as e:\n",
        "        print(\"graph_full.graphml present mais lecture impossible:\", repr(e))\n",
        "        graph = None\n",
        "else:\n",
        "    print(\"graph_full.graphml absent\")\n",
        "\n",
        "display(df_top.head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-match SIMBAD sur top anomalies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Simbad.add_votable_fields(\"otype\", \"main_id\", \"sp_type\", \"flux(B)\", \"flux(V)\")\n",
        "\n",
        "print(\"Cross-match SIMBAD (rayon 10 arcsec)...\")\n",
        "matches = []\n",
        "\n",
        "# On s'assure que ra/dec existent\n",
        "if \"ra\" not in df_top.columns or \"dec\" not in df_top.columns:\n",
        "    raise ValueError(\"df_top doit contenir les colonnes ra et dec (en degres).\")\n",
        "\n",
        "for _, row in df_top.iterrows():\n",
        "    coord = SkyCoord(ra=float(row[\"ra\"]) * u.deg, dec=float(row[\"dec\"]) * u.deg)\n",
        "    try:\n",
        "        res = Simbad.query_region(coord, radius=10 * u.arcsec)\n",
        "        if res is not None and len(res) > 0:\n",
        "            # Convertit RA/DEC SIMBAD vers SkyCoord pour calcul distance\n",
        "            simbad_coords = SkyCoord(res[\"RA\"], res[\"DEC\"], unit=(u.hourangle, u.deg))\n",
        "            dists = coord.separation(simbad_coords).arcsec\n",
        "            idx = int(np.argmin(dists))\n",
        "            match = {\n",
        "                \"source_id\": str(row.get(\"source_id\", \"\")),\n",
        "                \"score\": float(row.get(\"anomaly_score\", np.nan)),\n",
        "                \"simbad_id\": str(res[\"MAIN_ID\"][idx]),\n",
        "                \"otype\": str(res[\"OTYPE(V)\"][idx]),\n",
        "                \"sp_type\": str(res[\"SP_TYPE\"][idx]) if \"SP_TYPE\" in res.colnames else \"-\",\n",
        "                \"dist_arcsec\": float(np.round(dists[idx], 2)),\n",
        "                \"nb_matches\": int(len(res)),\n",
        "            }\n",
        "        else:\n",
        "            match = {\n",
        "                \"source_id\": str(row.get(\"source_id\", \"\")),\n",
        "                \"score\": float(row.get(\"anomaly_score\", np.nan)),\n",
        "                \"simbad_id\": \"\",\n",
        "                \"otype\": \"No match\",\n",
        "                \"sp_type\": \"\",\n",
        "                \"dist_arcsec\": np.nan,\n",
        "                \"nb_matches\": 0,\n",
        "            }\n",
        "    except Exception as e:\n",
        "        match = {\n",
        "            \"source_id\": str(row.get(\"source_id\", \"\")),\n",
        "            \"score\": float(row.get(\"anomaly_score\", np.nan)),\n",
        "            \"error\": repr(e),\n",
        "        }\n",
        "    matches.append(match)\n",
        "\n",
        "df_matches = pd.DataFrame(matches)\n",
        "df_matches = df_matches.sort_values(\"score\", ascending=False, na_position=\"last\")\n",
        "display(df_matches)\n",
        "\n",
        "df_matches.to_csv(\"outputs/simbad_crossmatch.csv\", index=False)\n",
        "print(\"Ecrit: outputs/simbad_crossmatch.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisations statiques et interactives (Plotly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_fig(fig, name: str, png_dir=\"screenshots\", html_dir=\"outputs\", scale=3):\n",
        "    png_path = os.path.join(png_dir, f\"{name}.png\")\n",
        "    html_path = os.path.join(html_dir, f\"{name}.html\")\n",
        "    os.makedirs(png_dir, exist_ok=True)\n",
        "    os.makedirs(html_dir, exist_ok=True)\n",
        "\n",
        "    # HTML interactif\n",
        "    try:\n",
        "        fig.write_html(html_path, include_plotlyjs=\"cdn\")\n",
        "    except Exception as e:\n",
        "        print(\"write_html fail\", name, \":\", repr(e))\n",
        "\n",
        "    # PNG HD via kaleido\n",
        "    try:\n",
        "        fig.write_image(png_path, scale=scale)\n",
        "    except Exception as e:\n",
        "        print(\"write_image fail\", name, \":\", repr(e))\n",
        "\n",
        "    return png_path, html_path\n",
        "\n",
        "\n",
        "# 1) Histogramme des scores\n",
        "if df_scored is not None and \"anomaly_score\" in df_scored.columns:\n",
        "    fig_histo = px.histogram(\n",
        "        df_scored,\n",
        "        x=\"anomaly_score\",\n",
        "        nbins=50,\n",
        "        title=\"Distribution des scores d'anomalie\",\n",
        "    )\n",
        "    fig_histo.update_layout(bargap=0.1)\n",
        "    save_fig(fig_histo, \"histo_scores\")\n",
        "    fig_histo.show()\n",
        "else:\n",
        "    print(\"Pas de scored.csv ou colonne anomaly_score absente: skip histo.\")\n",
        "\n",
        "# 2) Top anomalies bar\n",
        "if df_top is not None and \"anomaly_score\" in df_top.columns:\n",
        "    df_top_sorted = df_top.sort_values(\"anomaly_score\", ascending=False).copy()\n",
        "    if \"source_id\" not in df_top_sorted.columns:\n",
        "        df_top_sorted[\"source_id\"] = [str(i) for i in range(len(df_top_sorted))]\n",
        "    fig_top = px.bar(\n",
        "        df_top_sorted,\n",
        "        x=\"source_id\",\n",
        "        y=\"anomaly_score\",\n",
        "        title=\"Top anomalies, score par source_id\",\n",
        "        hover_data=[c for c in df_top_sorted.columns if c not in (\"anomaly_score\",)],\n",
        "    )\n",
        "    fig_top.update_layout(xaxis_tickangle=-45)\n",
        "    save_fig(fig_top, \"top_anomalies_bar\")\n",
        "    fig_top.show()\n",
        "else:\n",
        "    print(\"df_top/anomaly_score absent: skip top bar.\")\n",
        "\n",
        "# 3) Hidden Constellations interactif (RA/DEC)\n",
        "if df_top is not None and \"ra\" in df_top.columns and \"dec\" in df_top.columns:\n",
        "    fig_const = px.scatter(\n",
        "        df_top,\n",
        "        x=\"ra\",\n",
        "        y=\"dec\",\n",
        "        color=\"anomaly_score\" if \"anomaly_score\" in df_top.columns else None,\n",
        "        size=\"anomaly_score\" if \"anomaly_score\" in df_top.columns else None,\n",
        "        hover_data=[c for c in df_top.columns],\n",
        "        title=\"Hidden Constellations (interactive)\",\n",
        "    )\n",
        "    fig_const.update_traces(marker=dict(line=dict(width=1)))\n",
        "    save_fig(fig_const, \"hidden_constellations\")\n",
        "    fig_const.show()\n",
        "else:\n",
        "    print(\"df_top doit contenir ra/dec: skip hidden constellations.\")\n",
        "\n",
        "# 4) Embeddings UMAP et t-SNE (si df_scored existe)\n",
        "if df_scored is not None:\n",
        "    # Selection features simple\n",
        "    candidate_cols = [\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\", \"phot_g_mean_mag\", \"distance\"]\n",
        "    cols = [c for c in candidate_cols if c in df_scored.columns]\n",
        "    if len(cols) >= 2 and \"anomaly_score\" in df_scored.columns:\n",
        "        X = df_scored[cols].fillna(0.0).to_numpy(dtype=float)\n",
        "        y = df_scored[\"anomaly_score\"].to_numpy(dtype=float)\n",
        "\n",
        "        # UMAP\n",
        "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "        emb_umap = reducer.fit_transform(X)\n",
        "        df_umap = pd.DataFrame(emb_umap, columns=[\"dim1\", \"dim2\"])\n",
        "        df_umap[\"score\"] = y\n",
        "        fig_umap = px.scatter(\n",
        "            df_umap,\n",
        "            x=\"dim1\",\n",
        "            y=\"dim2\",\n",
        "            color=\"score\",\n",
        "            title=\"Cosmic cloud embedding (UMAP)\",\n",
        "        )\n",
        "        save_fig(fig_umap, \"umap_cosmic_cloud\")\n",
        "        fig_umap.show()\n",
        "\n",
        "        # t-SNE (peut etre lent si gros N)\n",
        "        n_tsne = min(len(X), 2000)\n",
        "        X_small = X[:n_tsne]\n",
        "        y_small = y[:n_tsne]\n",
        "        tsne = TSNE(n_components=2, random_state=42, init=\"pca\", learning_rate=\"auto\")\n",
        "        emb_tsne = tsne.fit_transform(X_small)\n",
        "        df_tsne = pd.DataFrame(emb_tsne, columns=[\"dim1\", \"dim2\"])\n",
        "        df_tsne[\"score\"] = y_small\n",
        "        fig_tsne = px.scatter(\n",
        "            df_tsne,\n",
        "            x=\"dim1\",\n",
        "            y=\"dim2\",\n",
        "            color=\"score\",\n",
        "            title=f\"Embedding t-SNE (n={n_tsne})\",\n",
        "        )\n",
        "        save_fig(fig_tsne, \"tsne_embedding\")\n",
        "        fig_tsne.show()\n",
        "    else:\n",
        "        print(\"Colonnes features insuffisantes ou anomaly_score absent: skip embeddings.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphe: degree distribution + preview interactif (si graph_full.graphml present)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if graph is None:\n",
        "    print(\"Pas de graphe charge: skip graph plots.\")\n",
        "else:\n",
        "    # Degree distribution\n",
        "    degrees = [d for _, d in graph.degree()]\n",
        "    df_deg = pd.DataFrame({\"degree\": degrees})\n",
        "    fig_deg = px.histogram(df_deg, x=\"degree\", nbins=60, title=\"Degree distribution (graph_full)\")\n",
        "    save_fig(fig_deg, \"graph_degree_distribution\")\n",
        "    fig_deg.show()\n",
        "\n",
        "    # Scatter: degree vs score (si mapping possible)\n",
        "    if df_scored is not None and \"source_id\" in df_scored.columns and \"anomaly_score\" in df_scored.columns:\n",
        "        # GraphML nodes: souvent string. On tente un alignement simple.\n",
        "        node_set = set(str(n) for n in graph.nodes())\n",
        "        tmp = df_scored.copy()\n",
        "        tmp[\"node_key\"] = tmp[\"source_id\"].astype(str)\n",
        "        tmp = tmp[tmp[\"node_key\"].isin(node_set)]\n",
        "        if len(tmp) > 0:\n",
        "            deg_map = {str(n): int(d) for n, d in graph.degree()}\n",
        "            tmp[\"degree\"] = tmp[\"node_key\"].map(deg_map).fillna(0).astype(int)\n",
        "            fig_ds = px.scatter(tmp, x=\"degree\", y=\"anomaly_score\", title=\"Degree vs anomaly_score\", hover_data=[\"source_id\"])\n",
        "            save_fig(fig_ds, \"degree_vs_score\")\n",
        "            fig_ds.show()\n",
        "        else:\n",
        "            print(\"Aucun alignement node<->source_id trouve: skip degree_vs_score.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optionnel: smoke-test torch-geometric (si installe et si graph + df_scored dispo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if not HAS_PYG:\n",
        "    print(\"torch-geometric non disponible: skip.\")\n",
        "elif graph is None or df_scored is None:\n",
        "    print(\"graph ou df_scored manquant: skip.\")\n",
        "else:\n",
        "    # Construction Data pyg simple: edge_index + features\n",
        "    # Node order: list(graph.nodes())\n",
        "    nodes = [str(n) for n in graph.nodes()]\n",
        "    node_index = {n: i for i, n in enumerate(nodes)}\n",
        "\n",
        "    # edge_index\n",
        "    edges = []\n",
        "    for u, v in graph.edges():\n",
        "        su, sv = str(u), str(v)\n",
        "        if su in node_index and sv in node_index:\n",
        "            edges.append((node_index[su], node_index[sv]))\n",
        "            edges.append((node_index[sv], node_index[su]))\n",
        "    if len(edges) == 0:\n",
        "        print(\"Aucune edge exploitable: skip.\")\n",
        "    else:\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # Features: selection simple depuis df_scored, alignement par source_id\n",
        "        candidate_cols = [\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\", \"phot_g_mean_mag\", \"distance\"]\n",
        "        cols = [c for c in candidate_cols if c in df_scored.columns]\n",
        "        if \"source_id\" not in df_scored.columns or len(cols) == 0:\n",
        "            print(\"Pas de source_id ou features: skip.\")\n",
        "        else:\n",
        "            feat_map = {}\n",
        "            for _, r in df_scored[[\"source_id\"] + cols].iterrows():\n",
        "                feat_map[str(r[\"source_id\"])] = [float(r[c]) if pd.notna(r[c]) else 0.0 for c in cols]\n",
        "\n",
        "            X = np.zeros((len(nodes), len(cols)), dtype=np.float32)\n",
        "            missing = 0\n",
        "            for n, i in node_index.items():\n",
        "                if n in feat_map:\n",
        "                    X[i] = np.array(feat_map[n], dtype=np.float32)\n",
        "                else:\n",
        "                    missing += 1\n",
        "            print(\"Features shape:\", X.shape, \"missing:\", missing)\n",
        "\n",
        "            data = Data(x=torch.tensor(X), edge_index=edge_index)\n",
        "\n",
        "            # Mini forward GAT (pas un entrainement complet)\n",
        "            conv = GATConv(in_channels=data.num_features, out_channels=32, heads=2, concat=True)\n",
        "            with torch.no_grad():\n",
        "                out = conv(data.x, data.edge_index)\n",
        "            print(\"GAT output:\", out.shape)\n",
        "\n",
        "            # Embedding plot (UMAP) sur un subset pour tester\n",
        "            emb = out.cpu().numpy()\n",
        "            n_plot = min(len(emb), 3000)\n",
        "            reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "            emb2 = reducer.fit_transform(emb[:n_plot])\n",
        "            df_gat = pd.DataFrame(emb2, columns=[\"dim1\", \"dim2\"])\n",
        "            fig_gat = px.scatter(df_gat, x=\"dim1\", y=\"dim2\", title=f\"GAT embedding UMAP (n={n_plot})\")\n",
        "            save_fig(fig_gat, \"gat_umap_embedding\")\n",
        "            fig_gat.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Export ZIP de tous les outputs...\")\n",
        "zip_name = \"outputs_all.zip\"\n",
        "if os.path.exists(zip_name):\n",
        "    os.remove(zip_name)\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for folder in [\"outputs\", \"screenshots\", \"screenshots_interactive\"]:\n",
        "        if not os.path.isdir(folder):\n",
        "            continue\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for f in files:\n",
        "                p = os.path.join(root, f)\n",
        "                arc = p\n",
        "                z.write(p, arcname=arc)\n",
        "\n",
        "print(\"OK:\", zip_name)\n",
        "\n",
        "# Colab download (decommente)\n",
        "# from google.colab import files\n",
        "# files.download(zip_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "AstroGraphAnomaly_Test_Maximal_2026.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}