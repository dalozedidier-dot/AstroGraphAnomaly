{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AstroGraphAnomaly \u2014 Colab Full Test (toutes capacit\u00e9s)\n\nObjectif : ex\u00e9cuter et **valider** un maximum de capacit\u00e9s du workflow, en restant **workflow-first** (CLI), sans supposer un mode package.\n\nCouverture :\n- ex\u00e9cution offline (CSV test) + jeux de donn\u00e9es synth\u00e9tiques\n- multi-engines + multi-strat\u00e9gies de seuil (avec fallback si option non support\u00e9e)\n- g\u00e9n\u00e9ration des artefacts (raw/scored/top/GraphML/manifest)\n- export de plots (set \u201cpertinent + beau\u201d)\n- explainability (LIME) + prompts LLM (lecture/visualisation)\n- analyses graph avanc\u00e9es (communaut\u00e9s, k-core, betweenness approx, articulation, bridges)\n- comparaison inter-runs (Jaccard overlap top anomalies + corr\u00e9lation scores)\n\n**Gaia** est optionnel (r\u00e9seau/quota). Le notebook ne le lance pas par d\u00e9faut.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/dalozedidier-dot/AstroGraphAnomaly.git\n",
        "%cd AstroGraphAnomaly\n",
        "!python -m pip install -q --upgrade pip\n",
        "!pip -q install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) D\u00e9tection entrypoint + capacit\u00e9s CLI\n\nOn d\u00e9tecte `workflow.py` ou `run_workflow.py` et on r\u00e9cup\u00e8re l\u2019aide CLI pour savoir ce qui est support\u00e9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, subprocess, re\n",
        "from pathlib import Path\n",
        "\n",
        "ENTRYPOINT = None\n",
        "if Path('workflow.py').exists():\n",
        "    ENTRYPOINT = 'workflow.py'\n",
        "elif Path('run_workflow.py').exists():\n",
        "    ENTRYPOINT = 'run_workflow.py'\n",
        "else:\n",
        "    raise FileNotFoundError('Aucun entrypoint trouv\u00e9: workflow.py ou run_workflow.py')\n",
        "\n",
        "print('Entrypoint d\u00e9tect\u00e9:', ENTRYPOINT)\n",
        "\n",
        "def _help_text():\n",
        "    try:\n",
        "        out = subprocess.check_output([sys.executable, ENTRYPOINT, '--help'], stderr=subprocess.STDOUT, text=True)\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        print('WARN: unable to read --help:', e)\n",
        "        return ''\n",
        "\n",
        "HELP = _help_text()\n",
        "print(HELP[:1200])\n",
        "\n",
        "# Try to infer supported engines/threshold strategies (best-effort)\n",
        "engines = []\n",
        "m = re.search(r\"--engine\\s+\\{([^}]+)\\}\", HELP)\n",
        "if m:\n",
        "    engines = [x.strip() for x in m.group(1).split(',')]\n",
        "else:\n",
        "    engines = ['isolation_forest', 'lof', 'ocsvm', 'robust_zscore']\n",
        "\n",
        "thr = []\n",
        "m2 = re.search(r\"--threshold-strategy\\s+\\{([^}]+)\\}\", HELP)\n",
        "if m2:\n",
        "    thr = [x.strip() for x in m2.group(1).split(',')]\n",
        "else:\n",
        "    thr = ['top_k', 'percentile', 'contamination']\n",
        "\n",
        "print('Candidate engines:', engines)\n",
        "print('Candidate threshold strategies:', thr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Jeux de donn\u00e9es\n\n- `data/sample_gaia_like.csv` : fourni par le repo (offline)\n- `data/sample_gaia_like_with_bp_rp.csv` : version enrichie (pour activer CMD offline)\n- `data/hubble_like.csv` : dataset synth\u00e9tique (m\u00eame sch\u00e9ma minimal) pour tester le mode `hubble` si pr\u00e9sent, sinon `csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "base_csv = data_dir/'sample_gaia_like.csv'\n",
        "assert base_csv.exists(), f'Missing {base_csv}'\n",
        "\n",
        "df = pd.read_csv(base_csv)\n",
        "\n",
        "# Ensure bp_rp exists for offline CMD (synthetic if not provided)\n",
        "df_bp = df.copy()\n",
        "if 'bp_rp' not in df_bp.columns:\n",
        "    # synthetic color index around typical range [0.0, 3.5]\n",
        "    rng = np.random.default_rng(42)\n",
        "    df_bp['bp_rp'] = np.clip(rng.normal(loc=1.5, scale=0.6, size=len(df_bp)), -0.5, 4.5)\n",
        "\n",
        "(data_dir/'sample_gaia_like_with_bp_rp.csv').write_text(df_bp.to_csv(index=False), encoding='utf-8')\n",
        "\n",
        "# Synthetic 'hubble-like' dataset: reusing Gaia-like schema + a couple extra cols\n",
        "rng = np.random.default_rng(7)\n",
        "n = 1200\n",
        "hubble = pd.DataFrame({\n",
        "    'source_id': np.arange(900000000000000000, 900000000000000000 + n, dtype=np.int64),\n",
        "    'ra': rng.uniform(10.0, 11.0, size=n),\n",
        "    'dec': rng.uniform(20.0, 21.0, size=n),\n",
        "    'parallax': np.abs(rng.normal(0.8, 0.25, size=n)) + 0.05,\n",
        "    'pmra': rng.normal(-5.0, 2.0, size=n),\n",
        "    'pmdec': rng.normal(-6.0, 2.2, size=n),\n",
        "    'phot_g_mean_mag': rng.uniform(16.0, 23.0, size=n),\n",
        "    'bp_rp': np.clip(rng.normal(1.6, 0.7, size=n), -0.5, 4.5),\n",
        "})\n",
        "hubble['distance'] = 1000.0 / hubble['parallax']\n",
        "# extra cols typical in imaging\n",
        "hubble['flux'] = rng.lognormal(mean=2.0, sigma=0.6, size=n)\n",
        "hubble['snr'] = rng.uniform(5, 200, size=n)\n",
        "\n",
        "(data_dir/'hubble_like.csv').write_text(hubble.to_csv(index=False), encoding='utf-8')\n",
        "\n",
        "print('Created:', data_dir/'sample_gaia_like_with_bp_rp.csv')\n",
        "print('Created:', data_dir/'hubble_like.csv')\n",
        "df_bp.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Runner robuste (fallback)\n\nOn tente une configuration riche (engine + threshold + features extended + plots + explain).\nSi l\u2019entrypoint refuse un flag (option non support\u00e9e), on retente avec un set minimal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import traceback\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    print('RUN:', ' '.join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "def run_workflow(mode: str, out_dir: str, in_csv: str = None, ra=None, dec=None, radius_deg=None, limit=None,\n",
        "                 engine='isolation_forest', threshold_strategy='top_k', top_k=30, explain_top=10, knn_k=8,\n",
        "                 features_mode='extended', plots=True):\n",
        "    outp = Path(out_dir)\n",
        "    outp.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Build \"rich\" command\n",
        "    if ENTRYPOINT == 'workflow.py':\n",
        "        cmd = [sys.executable, ENTRYPOINT, mode]\n",
        "        if mode in ('csv','hubble'):\n",
        "            cmd += ['--in-csv', in_csv]\n",
        "        if mode == 'gaia':\n",
        "            cmd += ['--ra', str(ra), '--dec', str(dec), '--radius-deg', str(radius_deg), '--limit', str(limit)]\n",
        "        cmd += ['--out', out_dir]\n",
        "        # optional flags\n",
        "        cmd += ['--engine', engine]\n",
        "        cmd += ['--threshold-strategy', threshold_strategy]\n",
        "        cmd += ['--top-k', str(top_k)]\n",
        "        cmd += ['--explain-top', str(explain_top)]\n",
        "        cmd += ['--knn-k', str(knn_k)]\n",
        "        cmd += ['--features-mode', features_mode]\n",
        "        if plots:\n",
        "            cmd.append('--plots')\n",
        "    else:\n",
        "        cmd = [sys.executable, ENTRYPOINT, '--mode', mode]\n",
        "        if mode in ('csv','hubble'):\n",
        "            cmd += ['--in-csv', in_csv]\n",
        "        if mode == 'gaia':\n",
        "            cmd += ['--ra', str(ra), '--dec', str(dec), '--radius-deg', str(radius_deg), '--limit', str(limit)]\n",
        "        cmd += ['--out', out_dir]\n",
        "        cmd += ['--engine', engine]\n",
        "        cmd += ['--threshold-strategy', threshold_strategy]\n",
        "        cmd += ['--top-k', str(top_k)]\n",
        "        cmd += ['--explain-top', str(explain_top)]\n",
        "        cmd += ['--knn-k', str(knn_k)]\n",
        "        cmd += ['--features-mode', features_mode]\n",
        "        if plots:\n",
        "            cmd.append('--plots')\n",
        "\n",
        "    # Try rich, fallback to minimal if needed\n",
        "    try:\n",
        "        run_cmd(cmd)\n",
        "        return {'status':'ok','cmd':cmd}\n",
        "    except Exception as e:\n",
        "        print('WARN: rich run failed, fallback minimal. Error:', e)\n",
        "        # Minimal: drop advanced flags\n",
        "        if ENTRYPOINT == 'workflow.py':\n",
        "            cmd2 = [sys.executable, ENTRYPOINT, mode]\n",
        "            if mode in ('csv','hubble'):\n",
        "                cmd2 += ['--in-csv', in_csv]\n",
        "            if mode == 'gaia':\n",
        "                cmd2 += ['--ra', str(ra), '--dec', str(dec), '--radius-deg', str(radius_deg), '--limit', str(limit)]\n",
        "            cmd2 += ['--out', out_dir]\n",
        "            cmd2 += ['--top-k', str(top_k)]\n",
        "            if plots:\n",
        "                cmd2.append('--plots')\n",
        "        else:\n",
        "            cmd2 = [sys.executable, ENTRYPOINT, '--mode', mode]\n",
        "            if mode in ('csv','hubble'):\n",
        "                cmd2 += ['--in-csv', in_csv]\n",
        "            if mode == 'gaia':\n",
        "                cmd2 += ['--ra', str(ra), '--dec', str(dec), '--radius-deg', str(radius_deg), '--limit', str(limit)]\n",
        "            cmd2 += ['--out', out_dir]\n",
        "            cmd2 += ['--top-k', str(top_k)]\n",
        "            if plots:\n",
        "                cmd2.append('--plots')\n",
        "\n",
        "        run_cmd(cmd2)\n",
        "        return {'status':'fallback_ok','cmd':cmd2,'rich_error':str(e)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Offline matrix : engines \u00d7 seuils (sur CSV enrichi bp_rp)\n\nOn utilise `sample_gaia_like_with_bp_rp.csv` pour activer le CMD offline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IN_CSV = 'data/sample_gaia_like_with_bp_rp.csv'\n",
        "OUT_BASE = Path('results/fulltest_offline')\n",
        "OUT_BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RUNS = []\n",
        "for eng in engines:\n",
        "    for t in thr:\n",
        "        out_dir = OUT_BASE / f'{eng}__{t}'\n",
        "        meta = run_workflow(\n",
        "            mode='csv',\n",
        "            in_csv=IN_CSV,\n",
        "            out_dir=str(out_dir),\n",
        "            engine=eng,\n",
        "            threshold_strategy=t,\n",
        "            top_k=30,\n",
        "            explain_top=10,\n",
        "            knn_k=8,\n",
        "            features_mode='extended',\n",
        "            plots=True,\n",
        "        )\n",
        "        RUNS.append({'engine':eng,'threshold':t,'out_dir':str(out_dir), **meta})\n",
        "\n",
        "runs_df = pd.DataFrame(RUNS)\n",
        "runs_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Validation des artefacts\n\nOn v\u00e9rifie pr\u00e9sence des fichiers attendus (raw/scored/top/graph/manifest + plots + explanations/prompt si disponibles).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "EXPECTED = [\n",
        "  'raw.csv',\n",
        "  'scored.csv',\n",
        "  'top_anomalies.csv',\n",
        "  'graph_full.graphml',\n",
        "  'graph_topk.graphml',\n",
        "  'manifest.json',\n",
        "]\n",
        "\n",
        "def check_out(out_dir: str):\n",
        "    out = Path(out_dir)\n",
        "    present = {p.name for p in out.glob('*')}\n",
        "    ok = all((out/e).exists() for e in EXPECTED)\n",
        "    plots = sorted([p.name for p in (out/'plots').glob('*.png')]) if (out/'plots').exists() else []\n",
        "    return {\n",
        "        'ok': ok,\n",
        "        'missing': [e for e in EXPECTED if not (out/e).exists()],\n",
        "        'n_plots': len(plots),\n",
        "        'has_cmd': (out/'plots'/'cmd_bp_rp_vs_g.png').exists(),\n",
        "        'has_explanations': (out/'explanations.jsonl').exists(),\n",
        "        'has_prompts': (out/'llm_prompts.jsonl').exists(),\n",
        "    }\n",
        "\n",
        "checks = []\n",
        "for r in RUNS:\n",
        "    c = check_out(r['out_dir'])\n",
        "    checks.append({**r, **c})\n",
        "checks_df = pd.DataFrame(checks)\n",
        "checks_df[['engine','threshold','status','ok','n_plots','has_cmd','has_explanations','has_prompts','missing']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Comparaison inter-runs\n\n- Overlap Jaccard des `top_anomalies`.\n- Corr\u00e9lation de score d\u2019anomalie sur l\u2019intersection des `source_id`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_top(out_dir: str):\n",
        "    top = pd.read_csv(Path(out_dir)/'top_anomalies.csv')\n",
        "    return set(top['source_id'].astype(str).tolist())\n",
        "\n",
        "def load_scores(out_dir: str):\n",
        "    df = pd.read_csv(Path(out_dir)/'scored.csv')[['source_id','anomaly_score']]\n",
        "    df['source_id'] = df['source_id'].astype(str)\n",
        "    return df\n",
        "\n",
        "labels = [f\"{r['engine']}|{r['threshold']}\" for r in RUNS]\n",
        "tops = [load_top(r['out_dir']) for r in RUNS]\n",
        "\n",
        "J = np.zeros((len(RUNS), len(RUNS)), dtype=float)\n",
        "for i in range(len(RUNS)):\n",
        "    for j in range(len(RUNS)):\n",
        "        inter = len(tops[i].intersection(tops[j]))\n",
        "        union = len(tops[i].union(tops[j]))\n",
        "        J[i,j] = inter/union if union else 0.0\n",
        "\n",
        "plt.figure(figsize=(max(6, len(RUNS)*0.6), max(5, len(RUNS)*0.5)))\n",
        "plt.imshow(J, aspect='auto')\n",
        "plt.xticks(range(len(labels)), labels, rotation=90)\n",
        "plt.yticks(range(len(labels)), labels)\n",
        "plt.title('Jaccard overlap \u2014 top anomalies')\n",
        "plt.colorbar(label='Jaccard')\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Score correlation (pairwise) on intersection of source_id\n",
        "Corr = np.full((len(RUNS), len(RUNS)), np.nan, dtype=float)\n",
        "score_dfs = [load_scores(r['out_dir']) for r in RUNS]\n",
        "\n",
        "for i in range(len(RUNS)):\n",
        "    for j in range(len(RUNS)):\n",
        "        a = score_dfs[i]\n",
        "        b = score_dfs[j]\n",
        "        m = a.merge(b, on='source_id', suffixes=('_a','_b'))\n",
        "        if len(m) >= 50:\n",
        "            Corr[i,j] = np.corrcoef(m['anomaly_score_a'].to_numpy(float), m['anomaly_score_b'].to_numpy(float))[0,1]\n",
        "\n",
        "plt.figure(figsize=(max(6, len(RUNS)*0.6), max(5, len(RUNS)*0.5)))\n",
        "plt.imshow(Corr, aspect='auto', vmin=-1, vmax=1)\n",
        "plt.xticks(range(len(labels)), labels, rotation=90)\n",
        "plt.yticks(range(len(labels)), labels)\n",
        "plt.title('Corr\u00e9lation des scores \u2014 intersection source_id')\n",
        "plt.colorbar(label='Pearson r')\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Galerie des plots export\u00e9s (run choisi)\n\nChoisir un run qui a `ok=True` et le maximum de plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# pick best run = ok with max plots\n",
        "best = max(checks, key=lambda x: (x['ok'], x['n_plots']))\n",
        "BEST_DIR = Path(best['out_dir'])\n",
        "print('BEST:', best['engine'], best['threshold'], 'status=', best['status'], 'plots=', best['n_plots'])\n",
        "\n",
        "plots_dir = BEST_DIR/'plots'\n",
        "for p in sorted(plots_dir.glob('*.png')):\n",
        "    print('PLOT:', p.name)\n",
        "    display(Image(filename=str(p)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Explainability (LIME) + prompts LLM\n\nLecture de `explanations.jsonl` et `llm_prompts.jsonl` si pr\u00e9sents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "exp_path = BEST_DIR/'explanations.jsonl'\n",
        "prm_path = BEST_DIR/'llm_prompts.jsonl'\n",
        "print('explanations:', exp_path.exists())\n",
        "print('prompts:', prm_path.exists())\n",
        "\n",
        "first_exp = None\n",
        "if exp_path.exists():\n",
        "    with exp_path.open('r', encoding='utf-8') as f:\n",
        "        first_exp = json.loads(next(f))\n",
        "    print('source_id:', first_exp.get('source_id'))\n",
        "    print('features (sample):', (first_exp.get('features') or {}) )\n",
        "    lime = (first_exp.get('lime') or {})\n",
        "    weights = lime.get('weights', [])\n",
        "    print('n weights:', len(weights))\n",
        "\n",
        "if first_exp and (first_exp.get('lime') or {}).get('weights'):\n",
        "    w = first_exp['lime']['weights']\n",
        "    w = sorted(w, key=lambda x: abs(x.get('weight', 0.0)), reverse=True)[:12]\n",
        "    labels_w = [x['feature'] for x in w]\n",
        "    vals_w = [x['weight'] for x in w]\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(11,5))\n",
        "    plt.bar(range(len(vals_w)), vals_w)\n",
        "    plt.xticks(range(len(vals_w)), labels_w, rotation=45, ha='right')\n",
        "    plt.title('LIME \u2014 top poids (premi\u00e8re anomalie expliqu\u00e9e)')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "if prm_path.exists():\n",
        "    with prm_path.open('r', encoding='utf-8') as f:\n",
        "        obj = json.loads(next(f))\n",
        "    print('Prompt for source_id:', obj.get('source_id'))\n",
        "    print(obj.get('prompt','')[:1800])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Analyse graphe avanc\u00e9e (GraphML)\n\n- communaut\u00e9s (Louvain si dispo, sinon greedy)\n- k-core\n- betweenness approx\n- articulation points + bridges\n- corr\u00e9lations avec anomaly_score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.read_graphml(BEST_DIR/'graph_full.graphml')\n",
        "print('Graph loaded:', G.number_of_nodes(), 'nodes', G.number_of_edges(), 'edges')\n",
        "\n",
        "nodes = list(G.nodes())\n",
        "deg = dict(G.degree())\n",
        "clust = nx.clustering(G)\n",
        "core = nx.core_number(G) if G.number_of_nodes() > 0 else {n:0 for n in nodes}\n",
        "\n",
        "# betweenness approx\n",
        "k = min(300, len(nodes))\n",
        "btw = nx.betweenness_centrality(G, k=k, normalized=True, seed=42) if len(nodes) > 1 else {n:0.0 for n in nodes}\n",
        "\n",
        "# communities\n",
        "try:\n",
        "    from networkx.algorithms.community import louvain_communities\n",
        "    comms = louvain_communities(G, seed=42)\n",
        "except Exception:\n",
        "    from networkx.algorithms.community import greedy_modularity_communities\n",
        "    comms = greedy_modularity_communities(G)\n",
        "\n",
        "comm_id = {}\n",
        "for i, cset in enumerate(comms):\n",
        "    for n in cset:\n",
        "        comm_id[n] = i\n",
        "\n",
        "aps = set(nx.articulation_points(G)) if G.number_of_nodes() > 2 else set()\n",
        "try:\n",
        "    bridges = list(nx.bridges(G))\n",
        "    bridge_nodes = set([a for a,b in bridges] + [b for a,b in bridges])\n",
        "except Exception:\n",
        "    bridge_nodes = set()\n",
        "\n",
        "gdf = pd.DataFrame({\n",
        "    'node': nodes,\n",
        "    'degree': [deg.get(n,0) for n in nodes],\n",
        "    'clustering': [clust.get(n,0.0) for n in nodes],\n",
        "    'kcore': [core.get(n,0) for n in nodes],\n",
        "    'betweenness': [btw.get(n,0.0) for n in nodes],\n",
        "    'community': [comm_id.get(n,-1) for n in nodes],\n",
        "    'is_articulation': [1 if n in aps else 0 for n in nodes],\n",
        "    'incident_to_bridge': [1 if n in bridge_nodes else 0 for n in nodes],\n",
        "})\n",
        "gdf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(9,5.5))\n",
        "plt.hist(gdf['degree'], bins=50, alpha=0.85)\n",
        "plt.title('Distribution degree')\n",
        "plt.xlabel('degree'); plt.ylabel('count')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,5.5))\n",
        "plt.hist(gdf['kcore'], bins=30, alpha=0.85)\n",
        "plt.title('Distribution k-core')\n",
        "plt.xlabel('kcore'); plt.ylabel('count')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,5.5))\n",
        "plt.hist(gdf['betweenness'], bins=60, alpha=0.85)\n",
        "plt.title('Distribution betweenness (approx)')\n",
        "plt.xlabel('betweenness'); plt.ylabel('count')\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Correlations with anomaly_score (if mapping exists)\n",
        "scored = pd.read_csv(BEST_DIR/'scored.csv')\n",
        "scored['source_id'] = scored['source_id'].astype(str)\n",
        "\n",
        "# graph nodes are often str; align by str\n",
        "gdf2 = gdf.copy()\n",
        "gdf2['source_id'] = gdf2['node'].astype(str)\n",
        "m = scored[['source_id','anomaly_score']].merge(gdf2[['source_id','degree','kcore','betweenness','community']], on='source_id', how='inner')\n",
        "print('Joined rows:', len(m))\n",
        "\n",
        "if len(m) > 50:\n",
        "    for col in ['degree','kcore','betweenness']:\n",
        "        plt.figure(figsize=(7,5))\n",
        "        plt.scatter(m[col].to_numpy(float), m['anomaly_score'].to_numpy(float), s=10, alpha=0.6)\n",
        "        plt.title(f'anomaly_score vs {col}')\n",
        "        plt.xlabel(col); plt.ylabel('anomaly_score')\n",
        "        plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Gaia (optionnel)\n\nPar d\u00e9faut : **d\u00e9sactiv\u00e9** (r\u00e9seau/quota). Pour activer : mettre `RUN_GAIA=1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if os.environ.get('RUN_GAIA','0') == '1':\n",
        "    out_gaia = 'results/fulltest_gaia'\n",
        "    run_workflow(\n",
        "        mode='gaia',\n",
        "        out_dir=out_gaia,\n",
        "        ra=266.4051,\n",
        "        dec=-28.936175,\n",
        "        radius_deg=0.3,\n",
        "        limit=800,\n",
        "        engine='isolation_forest',\n",
        "        threshold_strategy='top_k',\n",
        "        top_k=30,\n",
        "        explain_top=10,\n",
        "        knn_k=8,\n",
        "        features_mode='extended',\n",
        "        plots=True,\n",
        "    )\n",
        "    print('Gaia run done:', out_gaia)\n",
        "else:\n",
        "    print('Gaia skipped (set RUN_GAIA=1 to enable).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Hubble (support mentionn\u00e9)\n\nSi le mode `hubble` existe dans l\u2019entrypoint, on le teste avec `data/hubble_like.csv`. Sinon, on teste via `csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "has_hubble = False\n",
        "if ENTRYPOINT == 'workflow.py':\n",
        "    try:\n",
        "        # crude check: subcommands list in help\n",
        "        has_hubble = 'hubble' in HELP\n",
        "    except Exception:\n",
        "        has_hubble = False\n",
        "\n",
        "mode = 'hubble' if has_hubble else 'csv'\n",
        "out_h = 'results/fulltest_hubble_like'\n",
        "meta_h = run_workflow(\n",
        "    mode=mode,\n",
        "    in_csv='data/hubble_like.csv',\n",
        "    out_dir=out_h,\n",
        "    engine='robust_zscore',\n",
        "    threshold_strategy='percentile',\n",
        "    top_k=30,\n",
        "    explain_top=10,\n",
        "    knn_k=10,\n",
        "    features_mode='extended',\n",
        "    plots=True,\n",
        ")\n",
        "print('Hubble-like run mode:', mode)\n",
        "print('status:', meta_h['status'])\n",
        "print('out:', out_h)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Export du r\u00e9sum\u00e9\n\nOn \u00e9crit un r\u00e9sum\u00e9 machine-readable des runs offline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json, datetime\n",
        "\n",
        "summary_path = Path('results/fulltest_summary.json')\n",
        "summary_obj = {\n",
        "    'timestamp': datetime.datetime.now().isoformat(),\n",
        "    'entrypoint': ENTRYPOINT,\n",
        "    'runs': RUNS,\n",
        "    'checks': checks,\n",
        "}\n",
        "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "summary_path.write_text(json.dumps(summary_obj, indent=2), encoding='utf-8')\n",
        "print('Wrote:', summary_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab_full_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}